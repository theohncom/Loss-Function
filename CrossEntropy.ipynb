{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0555f47-9df7-4a4d-a049-33b34b6dacf9",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "### \"Negative logs on probabilities is what is known as the cross-entropy\"\n",
    "\n",
    "เนื้อหาส่วนใหญ่เรียบเรียงและแปลมาจาก What is Cross Entropy? https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f83313-e0db-4cf9-8d53-61f67db1c190",
   "metadata": {},
   "source": [
    "Cross-entropy loss ใช้ในการวัดประสิทธภาพของ classification model โดยผลที่ได้เป็นค่าความน่าจะเป็น(probability)ระหว่าง 0 ถึง 1 โดยค่า cross-entropy loss จะเปลี่ยนแปลงในแบบ log scale โดยหากความน่าจะเป็นของผมลัพธ์ที่ทำนาย(prediected probability)ได้ต่างจากค่าจริง(actual label)มากจะมากขึ้น\n",
    "\n",
    "จากรูปด้านล่าง ค่า cross-entropy loss ยิ่งเข้าใกล้ 1 ยิ่งไม่ดี ยิ่งเข้าใกล้ 0 ยิ่งดี\n",
    "ถ้า classsification model มั่นใจมากว่าถูกและให้ค่ามา predicted probability เป็น 1\n",
    "และเมื่อเปรียบเทียบกับค่าจริง(ผลเฉลย, actual label)แล้วถูกต้องจริงๆ จะได้ค่า loss เป็น 0 ซึ่งดีที่สุด\n",
    "ในทางกลับกันหากยิ่งมั่นใจว่าถูกแต่เมื่อเทียบกับผลเฉลยแล้วไม่ถูกก็จะได้ค่า loss ที่ยิ่งสูงมากขึ้น\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15972589-2322-4c05-ac2b-febeeba6f519",
   "metadata": {},
   "source": [
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png\" width=\"500\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b99bd-225e-44d9-b1ad-c541fdd7923f",
   "metadata": {},
   "source": [
    "#### Math\n",
    "\n",
    "ในกรณ๊จำแนกประเภทไบนารี(binary classification) โดยที่จำนวนคลาส(M)เท่ากับ 2 ครอสเอนโทรปีสามารถคำนวณได้ดังนี้:\n",
    "\n",
    "__$-{(y\\log(p) + (1 - y)\\log(1 - p))}$__\n",
    "\n",
    "หาก M>2 (เช่น การจำแนกประเภทหลายคลาส) เราจะคำนวณค่า loss แยกกันสำหรับแต่ละคลาสและรวมผลลัพธ์\n",
    "\n",
    "__$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$__\n",
    "\n",
    "\n",
    "___โดย___\n",
    "- M: number of classes (dog, cat, fish)\n",
    "- log: the natural log or ln\n",
    "- y: binary indicator (0 or 1) if class label $c$ is the correct classification for observation $o$\n",
    "- p: predicted probability observation $o$ is of class $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782154a-9a7d-4689-9139-a4af1fb0b700",
   "metadata": {},
   "source": [
    "จากรูปด่านล่างเป็นตัวอย่าง ผลลัพธ์จากสอง classification model: __ModelA__ และ __ModelB__ <br>\n",
    "จากภาพจะเห็นได้ว่า ModelB ให้ผลที่ดีกว่าทายถูกทั้ง 4 จุดและให้ความมั่นใจในการทายที่สูงทั้งหมด ยกเว้นสี่นำเงินหนึ่งจุดที่ไม่ค่อยมั่นใจให้มา 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a6ff8-375e-4c69-8fcc-29b48209a743",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1100/1*zg-mF9K3J2b5z7t76IYvlg.png\" width=\"600\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f61fc-d522-4fbe-bdf6-686c64ed9537",
   "metadata": {},
   "source": [
    "เราสามารถหาค่าความน่าจะเป็นรวมของแต่ละโมเดลได้ด้วยการคูณค่าความน่าจะเป็นของทุกผลลัพธ์ที่ได้<br>\n",
    "ModelA = <span style=\"color:red\">0.8 * 0.2 * </span><span style=\"color:blue\">0.4 * 0.6</span> = 0.0384 <br>\n",
    "ModelB = <span style=\"color:red\">0.8 * 0.6 * </span><span style=\"color:blue\">0.7 * 0.2</span> = 0.0627 <br>\n",
    "\n",
    "จากตรงนี้หากจำนวนผลการทำนายมีมากขึ้นค่าผลลัพธ์ความน่าจะเป็นของโมเดลจะยิ่งน้อยลงเรื่อยๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bfb8b1d-efec-4e98-a9b3-0abb490e521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value:  [0.93 0.42 0.65]\n",
      "multiply all elements:  0.25389 <============= less than previous result\n",
      "Value:  [0.62 0.91 0.55 0.57]\n",
      "multiply all elements:  0.1768767 <============= less than previous result\n",
      "Value:  [0.49 0.41 0.34 0.69 0.19]\n",
      "multiply all elements:  0.008954916600000001 <============= less than previous result\n",
      "Value:  [0.68 0.36 0.09 0.4  0.02 0.93]\n",
      "multiply all elements:  0.00016391808000000002 <============= less than previous result\n",
      "Value:  [0.06 0.95 0.23 0.43 0.64 0.63 0.84]\n",
      "multiply all elements:  0.0019092858623999999 <============= less than previous result\n",
      "Value:  [0.06 0.18 0.08 0.44 0.66 0.81 0.98 0.43]\n",
      "multiply all elements:  8.56426120704e-05 <============= less than previous result\n",
      "Value:  [0.81 0.74 0.97 0.22 0.55 0.65 0.12 0.26 0.65]\n",
      "multiply all elements:  0.000927374501196 <============= less than previous result\n",
      "Value:  [0.66 0.24 0.05 0.56 0.19 0.74 0.52 0.95 0.01 0.27]\n",
      "multiply all elements:  8.317431682560002e-07 <============= less than previous result\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(3,11):\n",
    "    narray = np.random.rand(i).round(2)\n",
    "    print(\"Value: \",narray)\n",
    "    print(\"multiply all elements: \", np.prod(narray), \"<============= less than previous result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53e9bb-0bba-4c17-8420-e5e80edfd84b",
   "metadata": {},
   "source": [
    "เราสามารถแก้ปัญหาได้ด้วยเปลี่ยนจากการคูณเป็นการบวกเเทน โดยใช้ log เข้ามาช่วย\n",
    "\n",
    "$log(a*b) = log(a)+log(b)$\n",
    "\n",
    "ModelA:<br>\n",
    "<span style=\"color:red\">0.8 * 0.2 * </span><span style=\"color:blue\">0.4 * 0.6</span> = 0.0384 <br>\n",
    "$ln(0.8*0.2*0.4*0.6) = ln(0.8)+ln(0.2)+ln(0.4)+ln(0.6)$ <br>\n",
    "$= (-0.22314355) + (-1.60943791) + (-0.91629073) + (-0.51082562)$<br> \n",
    "$= -3.259697819388456$ \n",
    "\n",
    "คูณด้วย -1 เพื่อให้ผลเป็นบวก (negative-log = cross-entropy)<br>\n",
    "$-1 * -3.259697819388456 = 3.259697819388456$ \n",
    "\n",
    "__Note__:  negative แล้วความหมายเป็นตรงกันข้ามทำให้ ค่ายิ่งสูงยิ่งไม่ดี\n",
    "\n",
    "ModelB = <span style=\"color:red\">0.8 * 0.6 * </span><span style=\"color:blue\">0.7 * 0.2</span> = 0.0627 <br>\n",
    "$ln(0.8*0.6*0.7*0.2) = ln(0.8)+ln(0.6)+ln(0.7)+ln(0.2)$ <br>\n",
    "$= (-0.0.22314355 ) + (-0.51082562  ) + (-0.35667494 ) + (-1.60943791)$<br> \n",
    "$= -2.700082031453033$ \n",
    "\n",
    "คูณด้วย -1 เพื่อให้ผลเป็นบวก (negative-log = cross-entropy)<br>\n",
    "$-1 * -2.700082031453033 = 2.700082031453033$ \n",
    "\n",
    "\n",
    "หากเราต้องการให้เป็นบวกตั้งแต่แรกก็ทำเป็น negative-log เลยโดยคุณ -1 จะได้สูตรเป็น <br>\n",
    "$-log(a*b) = -log(a)-log(b)$\n",
    "\n",
    "ซึ่งเมื่อเทียบกับสูตร cross-entropy ที่แสดงในตอนต้นก็จะพบว่าคลัายกันแล้ว<br>\n",
    "__$-{(y\\log(p) + (1 - y)\\log(1 - p))}$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a8123-5097-4ebe-af34-8f4b52e77587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae4f8e18-82f4-497e-9e92-f54713868824",
   "metadata": {},
   "source": [
    "__อีกหนึ่งตัวอย่าง__\n",
    "\n",
    "สมมุติให้มีประตูสามบานซึ่งอาจมีของขวัญอยู่หลังประตู<br>\n",
    "โมเดลทำการทำนายเป็น\n",
    "- ประตู 1 มีของขวัญเป็น p(gift) = 0.8\n",
    "- ประตู 2 มีของขวัญเป็น p(gift) = 0.7\n",
    "- ประตู 3 มีของขวัญเป็น p(gift)=0.1\n",
    "\n",
    "ความน่าจะเป็นรวมของโมเดลเป็น 0.8 * 0.7 * 0.1 = 0.065<br>\n",
    "ค่า negative log เป็น -ln(0.065) = 2.733\n",
    "\n",
    "ทดลองกลับผลของประตูที่สามเป็นความน่าจะเป็นที่ไม่มีของขวัญ จะได้ผลเป็น ดังรูปด่านล่าง<br>\n",
    "- ประตู 1 มีของขวัญเป็น p(gift) = 0.8\n",
    "- ประตู 2 มีของขวัญเป็น p(gift) = 0.7\n",
    "- ประตู 3 ไม่มีของขวัญเป็น p(no-gift)=(1-0.1)=0.9   คือทายว่าไม่มีของขวัญด้วยความมั่นใจ 0.9\n",
    "\n",
    "โดย Y คือผลเฉลย 1 คือมีของขวัญ และ 0 คือไม่มีของขวัญ หากดูจากผลเฉลยแล้วแสดงว่าโมเดลนี้ทายได้เม่นยำ <br>\n",
    "ดังนั้นค่า cross-entropy หรือค่า negative log ควรจะน้อยด้วย<br>\n",
    "ความน่าจะเป็นรวมของโมเดลเป็น 0.8 * 0.7 * 0.9 = 0.504<br>\n",
    "ค่า negative log เป็น -ln(0.504) = 0.685 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0035e-c05a-41f8-aec8-4c4bb69d5ced",
   "metadata": {},
   "source": [
    " <img src=\"https://miro.medium.com/max/1100/1*cGFROKVmg39lwr-4XAsagA.png\" width=\"600\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2803b-3e25-404b-b3ef-40d526e7bb81",
   "metadata": {},
   "source": [
    "ในตัวอย่างประตูนี้ เป็น binary classification คือทำนายว่ามีหรือไม่มีของขวัญหลังประตูมาร่วมในการคำนวนดังสูตร <br>\n",
    "$-{(y\\log(p) + (1 - y)\\log(1 - p))}$\n",
    "\n",
    "ดังนั้นความแม่นยำในการแต่ละประตูจะเป็น -((มีของขวัญ)log(p)+(ไม่มีของขวัญ)log(1-p)) = -(Y)log(p)-(1-Y)log(1-p)\n",
    "- ประตูที่ 1: -(1)ln(0.8) - (1-1)ln(0.2) = 0.223\n",
    "- ประตูที่ 2: -(1)ln(0.7) - (1-1)ln(0.3) = 0.356\n",
    "- ประตูที่ 3: -(0)ln(0.1) - (1-0)ln(0.9) = 0.105\n",
    "\n",
    "แต่ประตูคือแต่ละ class ดังนั้นในกรณีนี้เป็น 3 classes(multiclass classification)<br><br>\n",
    "$-\\sum_{c=1}^My_ilog(p_i)+(1-y_i)log(1-p_i)$ <br><br>\n",
    "ซึ่งลดรูปแล้วจะได้ดังนี้<br><br>\n",
    "$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$\n",
    "\n",
    "\n",
    "ดังนั้นผลรวมของทุกประตูจะมีค่าเป็น 0.223+0.356+0.105 = __0.683__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d27a60-4e58-4e45-be77-06af12d00566",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is taken straight from the Udacity course, Deep Learning with PyTorch.\n",
    "import numpy as np\n",
    "# This function takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450130b-d018-4123-97cf-29b70a927059",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Refs\n",
    "1. What is Cross Entropy? https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616\n",
    "2. Loss Functions https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "3. StatQuest: Neural Networks Part 6: Cross Entropy https://www.youtube.com/watch?v=6ArSys5qHAU\n",
    "4. StatQuest: Entropy (for data science) Clearly Explained!!! https://youtu.be/YtebGVx-Fxw\n",
    "5. DL 27 Multi-Class Cross Entropy 2 Fix https://www.youtube.com/watch?v=keDswcqkees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8a5f0-f2ca-4ffc-9a07-67b736464d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
